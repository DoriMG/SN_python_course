{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving data\n",
    "Often your analysis will start with the loading of a dataset. Some of the most common file types of datasets are excel, csv, mat and npy. We're going to see how to load files of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .mat files\n",
    "If you 're used to working with Matlab, or if you're using Matlab for another part of your data collection or analysis, you might be using .mat files to save your data. .mat files have a very specific organisation that is most similar to a dictionary in Python. Each array of data has a name (key) and values. You need the scipy io (input/output) package to load them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import os # We will use this to properly make our path to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sio.loadmat(os.path.join('data', 'cellTypes.mat'))\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data is loaded into a dict. This always happens, even if you .mat-file only contains 1 array. That is because the it will also add '__header__', '__version__' and '__globals__' to the dict. These have information about the .mat-file. We're not going to worry to much about that now. Let's find out which datasets are in this .mat-file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__header__', '__version__', '__globals__', 'allPC_ps', 'allSeq_ps', 'allPCs', 'allSeqs'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are the ones after __globals__. You can access them the same way you would values in a 'normal' dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(74, 1)\n"
     ]
    }
   ],
   "source": [
    "all_pcs = data['allPCs']\n",
    "print(type(all_pcs))\n",
    "print(all_pcs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data is stored in a numpy array. You can now start your analysis the way you always would."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a dataset that you would like to save as a .mat-file, scipy also has a function for that. Your data does have to be in a dictionary always. You can either create the dictionary beforehand, or make one while you are saving out your data, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sio.savemat(os.path.join('data', 'all_pcs.mat'), {'all_pcs':all_pcs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv (comma separated values) is a common way of saving out data. .csv-files always have values that are separated by a delimiter, which can be a number of different things, but is most commonly a comma. A newline often indicates a new row, but this is also not necessarily true. You can input what the delimiter is when you are loading your data.\n",
    "\n",
    "#### reading and writing using csv package\n",
    "There are a couple of ways to load csv. The most basic is using the csv package. This packages contains a reader, which allows you to read your csv file row by row. You can use this to save the rows into a list or array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6144.436,\n",
       " 878.409,\n",
       " 372.03,\n",
       " 372.051,\n",
       " 373.168,\n",
       " 373.302,\n",
       " 373.339,\n",
       " 373.72,\n",
       " 372.374,\n",
       " 373.15]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loco_data = []\n",
    "with open(os.path.join('data','loco.csv'), newline='') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    next(csvreader)\n",
    "    for row in csvreader:\n",
    "        loco_data.append(float(row[1]))\n",
    "        \n",
    "loco_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above we use the keyword 'with'. You can use this when you are working with files. The file is opened in the first line ('open') and will automatically close after the with ends. This is a good trick to stop having too many files open, which takes up a lot of memory and locks it for any other purposes. \n",
    "\n",
    "We then use the csv.reader function, which opens an iterator. That means it can take steps through the file, but doesn't actually hold the entire file in its memory. Again, this is a good trick to save memory.\n",
    "\n",
    "We can use a for-loop on an iterator like we would on a list (which is secretly also a type of iterator). We then save each row to the loco_data list. Note that this file had a header, which is now also added to the list. You can skip it by using the 'next' function for the iterator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write csv files in the exact same way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(os.path.join('data', 'cleaned_loco.csv'), mode='w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    for row in loco_data:\n",
    "        csvwriter.writerow([row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we use the parameter 'mode' which tells us what we aim to do with the file we're opening. In this case, we are just writing to it (w), but we could also read (r), write and read (w+) or append (a). \n",
    "In the writerow function, we use row, but make it a list by putting square brackets around it. This is because the function tries to iterate through all the values in row and put each of them in its own column. Therefore row has to be an iterable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reading and writing using pandas\n",
    "You can also directly import csv-files into a pandas Dataframe using the 'read_csv' function. As above, you can tell it what the delimeter should be. However, this function has a bunch of extra options, whether the original file has a header nad what the names of the columns should be. You can find a full list here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=csv#pandas.read_csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6144.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>878.409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>372.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>372.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>373.168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Y\n",
       "X          \n",
       "1  6144.436\n",
       "2   878.409\n",
       "3   372.030\n",
       "4   372.051\n",
       "5   373.168"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loco_data = pd.read_csv(os.path.join('data', 'loco.csv'), delimiter=',', header=0, index_col=0)\n",
    "loco_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write to a csv-file using the function 'to_csv'. Again you can tell it the delimiter (sep), but you can also chose to write the header and the indices or not. Find all options here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loco_data.to_csv(os.path.join('data', 'cleaned_loco.csv'), sep=',', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reading and writing using numpy\n",
    "The last way to import a csv-file is to read it directly to a numpy array. For this you will need the 'genfromtxt' function. This function works on any text file (and a csv file is just a special text file). Again you can give it options such as the delimiter, wheter to skip the header and what to do with missing values. See a full list here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6144.436,   878.409,   372.03 , ..., 20811.674, 20811.088,\n",
       "       20810.377])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loco_data= np.genfromtxt(os.path.join('data', 'loco.csv'), delimiter=',',skip_header=1, usecols=(1))\n",
    "loco_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy again uses a function for text-files to save out csv files, 'savetxt'. You have to tell it to save into a csv in the filename. Again you can tell it what to use as delimeter, header, etc. You can find more information here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.savetxt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(os.path.join('data', 'cleaned_loco.csv'), loco_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excel files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an excel file, the best way to load it in is to save it as a csv file and then load it using one of the methods above. However, if you really want to use the excel file,  you can do so using pandas. The 'read_excel' function takes the sheet as input too, so you can load a specific sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>6144.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>878.409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>372.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>372.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>373.168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   X         Y\n",
       "0  1  6144.436\n",
       "1  2   878.409\n",
       "2  3   372.030\n",
       "3  4   372.051\n",
       "4  5   373.168"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file = pd.read_excel(io = os.path.join('data','loco.xlsx'), sheet='loco')\n",
    "data_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save out a pandas DataFram into an excel sheet using 'to_excel'. Again, I would recommend to choose a csv format, because it is more versatile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file.to_excel(os.path.join('data','cleaned_loco.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### npy files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has its own way of saving out numpy arrays. You can use the 'load' function to load these files. When you use the function 'save' it will simply save the array with a .npy extension.\n",
    "\n",
    "The function allows the option to 'pickle' your data, this is a way for python to serialize your data and save it. Usually this is not necessary and even discouraged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load(os.path.join('data','celltypes.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=uint8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(data))\n",
    "data[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(os.path.join('data','celltypes_saved.npy'), all_pcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other formats\n",
    "\n",
    "You might run into data that is not of any of the formats mentioned above. If this is the case, the best thing to do is google something like 'load X file python'. Usually there is a specific library to do this, or your type is a derivative of one of the above and you can use one of these functions to load your data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
